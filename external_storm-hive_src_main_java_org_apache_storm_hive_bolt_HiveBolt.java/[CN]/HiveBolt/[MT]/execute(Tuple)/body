{
  try {
    boolean forceFlush=false;
    if (TupleUtils.isTick(tuple)) {
      LOG.debug("TICK received! current batch status [{}/{}]",tupleBatch.size(),options.getBatchSize());
      forceFlush=true;
    }
 else {
      List<String> partitionVals=options.getMapper().mapPartitions(tuple);
      HiveEndPoint endPoint=HiveUtils.makeEndPoint(partitionVals,options);
      HiveWriter writer=getOrCreateWriter(endPoint);
      writer.write(options.getMapper().mapRecord(tuple));
      tupleBatch.add(tuple);
      if (tupleBatch.size() >= options.getBatchSize())       forceFlush=true;
    }
    if (forceFlush && !tupleBatch.isEmpty()) {
      flushAllWriters(true);
      LOG.info("acknowledging tuples after writers flushed ");
      for (      Tuple t : tupleBatch) {
        collector.ack(t);
      }
      tupleBatch.clear();
    }
  }
 catch (  SerializationError se) {
    LOG.info("Serialization exception occurred, tuple is acknowledged but not written to Hive.",tuple);
    this.collector.reportError(se);
    collector.ack(tuple);
  }
catch (  Exception e) {
    this.collector.reportError(e);
    collector.fail(tuple);
    for (    Tuple t : tupleBatch) {
      collector.fail(t);
    }
    tupleBatch.clear();
    abortAndCloseWriters();
  }
}
