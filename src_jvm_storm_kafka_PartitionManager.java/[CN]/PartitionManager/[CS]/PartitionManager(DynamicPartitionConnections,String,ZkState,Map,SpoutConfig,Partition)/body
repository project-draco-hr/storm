{
  _partition=id;
  _connections=connections;
  _spoutConfig=spoutConfig;
  _topologyInstanceId=topologyInstanceId;
  _consumer=connections.register(id.host,id.partition);
  _state=state;
  _stormConf=stormConf;
  String jsonTopologyId=null;
  Long jsonOffset=null;
  String path=committedPath();
  try {
    Map<Object,Object> json=_state.readJSON(path);
    LOG.info("Read partition information from: " + path + "  --> "+ json);
    if (json != null) {
      jsonTopologyId=(String)((Map<Object,Object>)json.get("topology")).get("id");
      jsonOffset=(Long)json.get("offset");
    }
  }
 catch (  Throwable e) {
    LOG.warn("Error reading and/or parsing at ZkNode: " + path,e);
  }
  if (jsonTopologyId == null || jsonOffset == null) {
    _committedTo=KafkaUtils.getOffset(_consumer,spoutConfig.topic,id.partition,spoutConfig);
    LOG.info("No partition information found, using configuration to determine offset");
  }
 else   if (!topologyInstanceId.equals(jsonTopologyId) && spoutConfig.forceFromStart) {
    _committedTo=KafkaUtils.getOffset(_consumer,spoutConfig.topic,id.partition,spoutConfig.startOffsetTime);
    LOG.info("Topology change detected and reset from start forced, using configuration to determine offset");
  }
 else {
    _committedTo=jsonOffset;
    LOG.info("Read last commit offset from zookeeper: " + _committedTo + "; old topology_id: "+ jsonTopologyId+ " - new topology_id: "+ topologyInstanceId);
  }
  LOG.info("Starting Kafka " + _consumer.host() + ":"+ id.partition+ " from offset "+ _committedTo);
  _emittedToOffset=_committedTo;
  _fetchAPILatencyMax=new CombinedMetric(new MaxMetric());
  _fetchAPILatencyMean=new ReducedMetric(new MeanReducer());
  _fetchAPICallCount=new CountMetric();
  _fetchAPIMessageCount=new CountMetric();
}
