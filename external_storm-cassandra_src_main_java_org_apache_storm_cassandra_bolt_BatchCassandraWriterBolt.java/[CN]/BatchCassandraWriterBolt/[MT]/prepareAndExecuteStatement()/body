{
  int size=queue.size();
  if (size > 0) {
    List<Tuple> inputs=new ArrayList<>(size);
    queue.drainTo(inputs);
    try {
      List<PairStatementTuple> psl=buildStatement(inputs);
      int sinceLastModified=updateAndGetSecondsSinceLastModified();
      LOG.debug(logPrefix() + String.format("Execute cql batches with %s statements after %s seconds",size,sinceLastModified));
      checkTimeElapsedSinceLastExec(sinceLastModified);
      GroupingBatchBuilder batchBuilder=new GroupingBatchBuilder(cassandraConfConfig.getBatchSizeRows(),psl);
      int batchSize=0;
      for (      PairBatchStatementTuples batch : batchBuilder) {
        LOG.debug(logPrefix() + String.format("Writing data to %s in batches of %s rows.",cassandraConfConfig.getKeyspace(),batch.getInputs().size()));
        getAsyncExecutor().execAsync(batch.getStatement(),batch.getInputs());
        batchSize++;
      }
      int pending=getAsyncExecutor().getPendingExec();
      if (pending > batchSize) {
        LOG.warn(logPrefix() + String.format("Currently pending tasks is superior to the number of submit batches(%s) : %s",batchSize,pending));
      }
    }
 catch (    Throwable r) {
      LOG.error(logPrefix() + "Error(s) occurred while preparing batch statements");
      getAsyncHandler().failure(r,inputs);
    }
  }
}
