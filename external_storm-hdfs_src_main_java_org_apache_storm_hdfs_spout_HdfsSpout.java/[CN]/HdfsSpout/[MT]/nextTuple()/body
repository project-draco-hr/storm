{
  LOG.debug("Next Tuple {}",spoutId);
  if (!retryList.isEmpty()) {
    LOG.debug("Sending from retry list");
    HdfsUtils.Pair<MessageId,List<Object>> pair=retryList.remove();
    emitData(pair.getValue(),pair.getKey());
    return;
  }
  if (ackEnabled && tracker.size() >= maxDuplicates) {
    LOG.warn("Waiting for more ACKs before generating new tuples. " + "Progress tracker size has reached limit {}, SpoutID {}",maxDuplicates,spoutId);
    return;
  }
  while (true) {
    try {
      if (reader == null) {
        reader=pickNextFile();
        if (reader == null) {
          LOG.debug("Currently no new files to process under : " + sourceDirPath);
          return;
        }
 else {
          fileReadCompletely=false;
        }
      }
      if (fileReadCompletely) {
        return;
      }
      List<Object> tuple=reader.next();
      if (tuple != null) {
        fileReadCompletely=false;
        ++tupleCounter;
        MessageId msgId=new MessageId(tupleCounter,reader.getFilePath(),reader.getFileOffset());
        emitData(tuple,msgId);
        if (!ackEnabled) {
          ++acksSinceLastCommit;
          commitProgress(reader.getFileOffset());
        }
 else {
          commitProgress(tracker.getCommitPosition());
        }
        return;
      }
 else {
        fileReadCompletely=true;
        if (!ackEnabled) {
          markFileAsDone(reader.getFilePath());
        }
      }
    }
 catch (    IOException e) {
      LOG.error("I/O Error processing at file location " + getFileProgress(reader),e);
      return;
    }
catch (    ParseException e) {
      LOG.error("Parsing error when processing at file location " + getFileProgress(reader) + ". Skipping remainder of file.",e);
      markFileAsBad(reader.getFilePath());
    }
  }
}
