{
  String topId=td.getId();
  LOG.debug("Topology {} is isolated",topId);
  int nodesFromUsAvailable=nodesAvailable();
  int nodesFromOthersAvailable=NodePool.nodesAvailable(lesserPools);
  int nodesUsed=_topologyIdToNodes.get(topId).size();
  int nodesNeeded=nodesRequested - nodesUsed;
  LOG.debug("Nodes... requested {} used {} available from us {} " + "avail from other {} needed {}",new Object[]{nodesRequested,nodesUsed,nodesFromUsAvailable,nodesFromOthersAvailable,nodesNeeded});
  if ((nodesNeeded - nodesFromUsAvailable) > (_maxNodes - _usedNodes)) {
    _cluster.setStatus(topId,"Max Nodes(" + _maxNodes + ") for this user would be exceeded. "+ ((nodesNeeded - nodesFromUsAvailable) - (_maxNodes - _usedNodes))+ " more nodes needed to run topology.");
    return 0;
  }
  int nodesNeededFromOthers=Math.min(Math.min(_maxNodes - _usedNodes,nodesFromOthersAvailable),nodesNeeded);
  int nodesNeededFromUs=nodesNeeded - nodesNeededFromOthers;
  LOG.debug("Nodes... needed from us {} needed from others {}",nodesNeededFromUs,nodesNeededFromOthers);
  if (nodesNeededFromUs > nodesFromUsAvailable) {
    _cluster.setStatus(topId,"Not Enough Nodes Available to Schedule Topology");
    return 0;
  }
  Collection<Node> found=NodePool.takeNodes(nodesNeededFromOthers,lesserPools);
  _usedNodes+=found.size();
  allNodes.addAll(found);
  Collection<Node> foundMore=takeNodes(nodesNeededFromUs);
  _usedNodes+=foundMore.size();
  allNodes.addAll(foundMore);
  int totalTasks=td.getExecutors().size();
  int origRequest=td.getNumWorkers();
  int slotsRequested=Math.min(totalTasks,origRequest);
  int slotsUsed=Node.countSlotsUsed(allNodes);
  int slotsFree=Node.countFreeSlotsAlive(allNodes);
  int slotsToUse=Math.min(slotsRequested - slotsUsed,slotsFree);
  if (slotsToUse <= 0) {
    _cluster.setStatus(topId,"Node has partially crashed, if this situation persists rebalance the topology.");
  }
  return slotsToUse;
}
